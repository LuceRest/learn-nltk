Pt 1 :

- Tokenizing → Tahap pemotongan string atau kalimat input berdasarkan tiap kata yang menyusunnya.

- word_tokenize() ~> Berfungsi untuk memisah suatu kalimat menjadi kata-kata (dipisahkan berdasarkan spasi).
- word_tokenize(<kalimat yg ingin dipisah>)

- sent_tokenize() ~> Berfungsi untuk memisahkan paragraf menjadi kalimat-kalimat (dipisahkan berdasarkan titik dan tanda baca).
- sent_tokenize(<paragraf yg ingin dipisah>)



Pt 2 :

- Stop Words disebut juga Filtering
- Filtering → Tahap pemilihan kata-kata penting dari hasil token, yaitu katakata apa saja yang akan digunakan untuk mewakili dokumen

- stopwords.words() ~> Berfungsi untuk mengambil data stopwords dengan bahasa tertentu.
- stopwords.words(<bahasa stopword>)



Pt 3 :

- Stemming → Tahap mencari kata dasar (root) dari setiap kata hasil filtering.

- PorterStemmer().stem() ~> Berfungsi untuk melakukan stemming pada suatu kata.
- PorterStemmer().stem(<word>)



Pt  4 :

- Tagging
    Tahap untuk mencari bentuk awal dari tiap kata lampau atau hasil dari stemming yang masih memuat beberapa kata lampau yang dikembalikan ke bentuk awalnya.
    
- Punkt Sentence Tokenizer
    Tokenizer ini membagi teks menjadi daftar kalimat, dengan menggunakan algoritma yang tidak diawasi untuk membangun model kata singkatan, kolokasi, dan kata yang memulai kalimat. Itu harus dilatih pada banyak koleksi plaintext dalam bahasa target sebelum dapat digunakan.

- PoS tagging → Proses memberikan label kelas kata secara otomatis pada setiap kata yang ada pada suatu teks atau dokumen.

- List of Universal POS tags:
    • ADJ : Adjective
    • ADV : Adposition
    • ADP : Adverb
    • AUX : Auxiliary
    • CCONJ : Coordinating Conjuction
    • DET : Determiner
    • INTJ : Interjection
    • NOUN : Noun
    • NUM : Numeral
    • PART : Particle
    • PRON : Pronoun
    • PROPN : Proper Noun
    • PUNCT : Punctuation
    • SCONJ : Subordinating Conjuction
    • SYM : Symbol
    • VERB : Verb
    • X : Other

- Another LIST OF TAGS : 
    • CC : coordinating conjunction
    • CD : cardinal digit
    • DT : determiner
    • EX : existential there (like: “there is” … think of it like “there exists”)
    • FW : foreign word
    • IN : preposition/subordinating conjunction
    • JJ : adjective ‘big’
    • JJR : adjective, comparative ‘bigger’
    • JJS : adjective, superlative ‘biggest’
    • LS : list marker 1)
    • MD : modal could, will
    • NN : noun, singular ‘desk’
    • NNS : noun plural ‘desks’
    • NNP : proper noun, singular ‘Harrison’
    • NNPS : proper noun, plural ‘Americans’
    • PDT : predeterminer ‘all the kids’
    • POS : possessive ending parent‘s
    • PRP : personal pronoun I, he, she
    • PRPS : possessive pronoun my, his, hers
    • RB : adverb very, silently,
    • RBR : adverb, comparative better
    • RBS : adverb, superlative best
    • RP : particle give up
    • TO : to go ‘to‘ the store.
    • UH : interjection errrrrrrrm
    • VB : verb, base form take
    • VBD : verb, past tense took
    • VBG : verb, gerund/present participle taking
    • VBN : verb, past participle taken
    • VBP : verb, sing. present, non-3d take
    • VBZ : verb, 3rd person sing. present takes
    • WDT : wh-determiner which
    • WP : wh-pronoun who, what
    • WPS : possessive wh-pronoun whose
    • WRB : wh-abverb where, when



Pt 5 :

- Proses chunking dilakukan untuk mendapatkan informasi mengenai struktur kalimat.
- Chunking              → Proses yang mana ketika informassi memasuki memori, ia dapat dikodekan ulang sehingga konsep-konsep yang terkait dapat dikelompokkan bersama menjadi satu potongan.
- Chunking ini sering digunakan sebagai teknik menghafal.

- chunk.RegexpParser    → Menggunakan satu set pola ekspresi reguler untuk menentukan perilaku parser.



Pt 7 :

- nltk.chunk.ne_chunk() ~> Berfungsi untuk memberi nama pada entity chunker untuk memotong (chunk) daftar token yang diberi tag.
- nltk.chunk.ne_chunk(<tagged tokens*>,* binary=False/True)
    - binary = False    ~> Berfungsi untuk menampilkan nama entity chunker.
    - binary = True     ~> Berfungsi untuk tidak menampilkan nama entity chunker.



Pt 8 :

- Lemmatization → Proses yang bertujuan untuk melakukan normalisasi pada teks/kata dengan berdasarkan pada bentuk dasar yang merupakan bentuk lemma-nya.
- Lemma         → Bentuk dasar dari sebuah kata yang memiliki arti tertentu berdasar pada kamus.

- WordNetLemmatizer().lemmatize()   ~> Berfungsi untuk melakukan lemmatizer pada suatu kata.
- WordNetLemmatizer().lemmatize(<word>, "<pos>")
    - Nilai default untuk pos       → pos=”n” (noun)



Pt 9 :

- Korpus                → Kumpulan teks autentik, baik tulis maupun transkrip percakapan dalam jumlah besar yang disimpan secara elektronik.
- Kelebihan korpus adalah mudah untuk diakses dan analisis berbasis korpus bisa dibuat generalisasi secara kuantitatif.
- Korpus data           → Data yang dipakai sebagai sumber bahan penelitian.
- Korpus (linguistik)   → Kumpulan ujaran yang tertulis atau lisan yang digunakan untuk menyokong atau menguji hipotesis tentang struktur bahasa.



Pt 10 :

- WordNet → Dabatase bahasa yang digunakan untuk mencari synonym set (synset) pada sebuah kata (lemma) yang nantinya akan berelasi dari satu lema dengan lemma lainnya.

- <word 1>.wup_similarity(<word 2>) ~> Berfungsi untuk mengcompare similarity (kesamaan) antara kata 1 dengan kata 2.



Pt 11 :

- random.shuffle()              ~> Berfungsi untuk mengacak nilai-nilai pada suatu list.
- random.shuffle(<list>)

- nltk.FreqDist().most_common() ~> Berfungsi untuk memunculkan nilai yg sering muncul.
- nltk.FreqDist(<list>).most_common(<banyak nilai yg ingin ditampilkan)



Pt 13 :

- nltk.NaiveBayesClassifier.train() ~> Berfungsi untuk melakukan training pada suatu dataset dengan menggunakan algoritma Naïve Bayes.
- nltk.NaiveBayesClassifier.train(<training dataset>)

- nltk.classify.accuracy()          ~> Berfungs untuk mendapatkan akurasi dari suatu klasifier dengan mengetes suatu dataset.
- nltk.classify.accuracy(<classifier>, <testing dataset>)

- show_most_informative_features()  ~> Berfungsi untuk menampilkan fitur paling informatif.
- show_most_informative_features(<jumlah data yg ingin ditampilkan>)



Pt 14 :

- Pickle 			~> Modul yang dapat digunakan untuk menyimpan dan membaca data ke dalam / dari sebuah file (menyimpan objek dengan python).

- open() 			~> Berfungsi untuk membuka suatu file.
- <nama variabel> = open(”<nama file>.<ekstensi file>”, “wb”)
    - wb → Write and binary

- pickle.dump()			~> Berfungsi untuk menyimpan (save) objek menjadi file .pickle.
- pickle.dump(<objek yg ingin disimpan>, <file .pickle>)

- pickle.load() 		~> Berfungsi untuk load objek yg telah disimpan menjadi file .pickle.
- <nama variabel pickle> = pickle.load(<file .pickle>)

- <nama variabel pickle>.close() ~> Berfungsi untuk menutup file .pickle yg telah digunakan.



Pt 15 :

- SklearnClassifier() ~> Berfungsi untuk memanggil suatu algoritma classifier.
- SklearnClassifier(<algoritma classifier>)



Pt 16 :

- statistics.mode() ~> Berfungsi untuk mengambil suatu data yang muncul paling banyak (modus).
- statistics.mode(<data>)



